#!/usr/bin/env python3
"""
BEV-Plannerì—ì„œ ì˜ê°ì„ ë°›ì€ ê°„ì†Œí™”ëœ ê²½ë¡œ ê³„íš ëª¨ë¸
YOLOP ì¶œë ¥ê³¼ ego ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•ˆì „í•œ ë¯¸ë˜ ê¶¤ì ì„ ìƒì„±
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import math


class SimplifiedBEVPlanner(nn.Module):
    """
    ê°„ì†Œí™”ëœ BEV ê¸°ë°˜ ê²½ë¡œ ê³„íš ëª¨ë¸
    
    ì£¼ìš” ê¸°ëŠ¥:
    1. BEV íŠ¹ì§• + Ego ìƒíƒœ â†’ Transformer ê¸°ë°˜ ì²˜ë¦¬
    2. ë¯¸ë˜ 6ìŠ¤í… ê¶¤ì  ì˜ˆì¸¡ (3ì´ˆ, 0.5ì´ˆ ê°„ê²©)
    3. ì¶©ëŒ íšŒí”¼ ë° ì°¨ì„  ìœ ì§€ ì œì•½
    """
    
    def __init__(self,
                 bev_embed_dim: int = 256,
                 ego_embed_dim: int = 256,
                 hidden_dim: int = 512,
                 num_future_steps: int = 6,
                 num_transformer_layers: int = 4,
                 num_attention_heads: int = 8,
                 dropout: float = 0.1,
                 max_speed: float = 15.0,
                 safety_margin: float = 2.0):
        
        super(SimplifiedBEVPlanner, self).__init__()
        
        self.bev_embed_dim = bev_embed_dim
        self.ego_embed_dim = ego_embed_dim
        self.hidden_dim = hidden_dim
        self.num_future_steps = num_future_steps
        self.max_speed = max_speed
        self.safety_margin = safety_margin
        
        # BEV íŠ¹ì§• ì••ì¶• (3840 -> ë” ì‘ì€ ì°¨ì›)
        self.bev_compressor = nn.Sequential(
            nn.Linear(bev_embed_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
        
        # Ego ìƒíƒœ ì²˜ë¦¬
        self.ego_processor = nn.Sequential(
            nn.Linear(ego_embed_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
        
        # íŠ¹ì§• ìœµí•© (hidden_dim//2 -> hidden_dim)
        fusion_input_dim = hidden_dim // 2  # BEVì™€ ego ëª¨ë‘ hidden_dim//2 ì°¨ì›
        self.feature_fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout)
        )
        
        # Transformer ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸ë§
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_attention_heads,
            dim_feedforward=hidden_dim * 2,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=num_transformer_layers
        )
        
        # ê¶¤ì  ë””ì½”ë” (ë¯¸ë˜ ê° ìŠ¤í…ì˜ (x, y) ì¢Œí‘œ ì˜ˆì¸¡)
        self.trajectory_decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_future_steps * 2)  # (x, y) * num_steps
        )
        
        # ì‹ ë¢°ë„ ì˜ˆì¸¡ (ê° ìŠ¤í…ì˜ ì˜ˆì¸¡ ì‹ ë¢°ë„)
        self.confidence_decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim // 2, num_future_steps),
            nn.Sigmoid()  # 0~1 ì‚¬ì´ ì‹ ë¢°ë„
        )
        
        # Learnable query tokens for future steps
        self.future_queries = nn.Parameter(
            torch.randn(1, num_future_steps, hidden_dim)
        )
        
    def forward(self, 
                bev_features: torch.Tensor,
                ego_features: torch.Tensor,
                return_attention: bool = False) -> Dict[str, torch.Tensor]:
        """
        Args:
            bev_features: [B, H*W, C] BEV ê³µê°„ íŠ¹ì§•
            ego_features: [B, C] Ego ì°¨ëŸ‰ ìƒíƒœ íŠ¹ì§•
            return_attention: attention weights ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            Dict containing:
                - 'trajectory': [B, num_steps, 2] ë¯¸ë˜ ê¶¤ì  (x, y)
                - 'confidence': [B, num_steps] ê° ìŠ¤í…ì˜ ì‹ ë¢°ë„
                - 'attention_weights': [B, num_heads, seq_len, seq_len] (optional)
        """
        batch_size = bev_features.size(0)
        
        # 1. BEV íŠ¹ì§• ì••ì¶• ë° ì¤‘ìš” ì˜ì—­ ì„ íƒ
        bev_compressed = self.bev_compressor(bev_features)  # [B, H*W, hidden_dim//2]
        
        # ìƒìœ„ Kê°œ ì¤‘ìš” BEV ìœ„ì¹˜ë§Œ ì„ íƒ (attention íš¨ìœ¨ì„±ì„ ìœ„í•´)
        K = min(64, bev_compressed.size(1))  # ìµœëŒ€ 64ê°œ ìœ„ì¹˜
        bev_importance = bev_compressed.norm(dim=-1)  # [B, H*W]
        _, top_indices = torch.topk(bev_importance, K, dim=-1)  # [B, K]
        
        # ì¤‘ìš” BEV íŠ¹ì§• ì¶”ì¶œ
        bev_selected = torch.gather(
            bev_compressed, 1, 
            top_indices.unsqueeze(-1).expand(-1, -1, bev_compressed.size(-1))
        )  # [B, K, hidden_dim//2]
        
        # 2. Ego íŠ¹ì§• ì²˜ë¦¬
        ego_processed = self.ego_processor(ego_features).unsqueeze(1)  # [B, 1, hidden_dim//2]
        
        # 3. íŠ¹ì§• ìœµí•©
        # BEVì™€ egoë¥¼ ê²°í•© [B, K+1, hidden_dim//2]
        combined_features = torch.cat([ego_processed, bev_selected], dim=1)  
        
        # ê° ìœ„ì¹˜ë³„ë¡œ feature_fusion ì ìš©í•˜ì—¬ hidden_dimìœ¼ë¡œ í™•ì¥
        B, seq_len, feat_dim = combined_features.shape
        combined_features = combined_features.view(-1, feat_dim)  # [B*(K+1), hidden_dim//2]
        combined_features = self.feature_fusion(combined_features)  # [B*(K+1), hidden_dim]
        combined_features = combined_features.view(B, seq_len, self.hidden_dim)  # [B, K+1, hidden_dim]
        
        # 4. Future query tokens ì¶”ê°€
        future_queries = self.future_queries.expand(batch_size, -1, -1)  # [B, num_steps, hidden_dim]
        full_sequence = torch.cat([combined_features, future_queries], dim=1)  # [B, K+1+num_steps, hidden_dim]
        
        # 5. Transformer ì²˜ë¦¬
        transformer_output = self.transformer_encoder(full_sequence)  # [B, seq_len, hidden_dim]
        
        # Future step outputs ì¶”ì¶œ (ë§ˆì§€ë§‰ num_future_stepsê°œ)
        future_outputs = transformer_output[:, -(self.num_future_steps):, :]  # [B, num_steps, hidden_dim]
        
        # 6. ê¶¤ì  ë° ì‹ ë¢°ë„ ë””ì½”ë”©
        # Global contextë¥¼ ìœ„í•´ í‰ê·  pooling
        global_context = transformer_output.mean(dim=1)  # [B, hidden_dim]
        
        # ê¶¤ì  ì˜ˆì¸¡ (ì „ì—­ì  ì ‘ê·¼)
        trajectory_flat = self.trajectory_decoder(global_context)  # [B, num_steps*2]
        trajectory = trajectory_flat.view(batch_size, self.num_future_steps, 2)  # [B, num_steps, 2]
        
        # ì‹ ë¢°ë„ ì˜ˆì¸¡
        confidence = self.confidence_decoder(global_context)  # [B, num_steps]
        
        # 7. ë¬¼ë¦¬ì  ì œì•½ ì ìš©
        trajectory = self._apply_physical_constraints(trajectory, ego_features)
        
        result = {
            'trajectory': trajectory,
            'confidence': confidence,
            'bev_attention_indices': top_indices  # ë””ë²„ê¹…ìš©
        }
        
        if return_attention:
            # Attention weights ì¶”ì¶œì€ ë³µì¡í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµ
            # ì‹¤ì œë¡œëŠ” transformerì˜ attention weightsë¥¼ ë”°ë¡œ ì €ì¥í•´ì•¼ í•¨
            pass
            
        return result
    
    def _apply_physical_constraints(self, 
                                  trajectory: torch.Tensor,
                                  ego_features: torch.Tensor) -> torch.Tensor:
        """
        ë¬¼ë¦¬ì  ì œì•½ì„ ì ìš©í•˜ì—¬ í˜„ì‹¤ì ì¸ ê¶¤ì ìœ¼ë¡œ ì¡°ì •
        
        Args:
            trajectory: [B, num_steps, 2] ì›ì‹œ ê¶¤ì  ì˜ˆì¸¡
            ego_features: [B, ego_dim] í˜„ì¬ ego ìƒíƒœ
            
        Returns:
            ì œì•½ì´ ì ìš©ëœ ê¶¤ì  [B, num_steps, 2]
        """
        batch_size, num_steps, _ = trajectory.shape
        
        # 1. ì†ë„ ì œì•½ (ì—°ì†ëœ ìŠ¤í… ê°„ì˜ ìµœëŒ€ ì´ë™ ê±°ë¦¬)
        dt = 0.5  # 0.5ì´ˆ ê°„ê²©
        max_step_distance = self.max_speed * dt  # ìµœëŒ€ ìŠ¤í…ë‹¹ ì´ë™ ê±°ë¦¬
        
        constrained_trajectory = trajectory.clone()
        
        for step in range(1, num_steps):
            # ì´ì „ ìŠ¤í…ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°
            step_delta = constrained_trajectory[:, step] - constrained_trajectory[:, step-1]
            step_distance = torch.norm(step_delta, dim=-1, keepdim=True)  # [B, 1]
            
            # ìµœëŒ€ ê±°ë¦¬ ì´ˆê³¼ ì‹œ ì •ê·œí™”
            exceed_mask = step_distance > max_step_distance
            if exceed_mask.any():
                normalized_delta = step_delta / (step_distance + 1e-8) * max_step_distance
                constrained_trajectory[:, step] = torch.where(
                    exceed_mask.expand(-1, 2),
                    constrained_trajectory[:, step-1] + normalized_delta,
                    constrained_trajectory[:, step]
                )
        
        # 2. ë¶€ë“œëŸ¬ìš´ ê¶¤ì ì„ ìœ„í•œ ìŠ¤ë¬´ë”© (ì„ íƒì )
        if num_steps >= 3:
            # ì¤‘ì•™ê°’ í•„í„°ë§ìœ¼ë¡œ ê¸‰ê²©í•œ ë³€í™” ì œê±°
            for step in range(1, num_steps-1):
                prev_pos = constrained_trajectory[:, step-1]
                curr_pos = constrained_trajectory[:, step]
                next_pos = constrained_trajectory[:, step+1]
                
                # ë¶€ë“œëŸ¬ìš´ ë³´ê°„
                smoothed_pos = 0.25 * prev_pos + 0.5 * curr_pos + 0.25 * next_pos
                constrained_trajectory[:, step] = smoothed_pos
        
        return constrained_trajectory
    
    def predict_trajectory(self,
                          bev_features: torch.Tensor,
                          ego_features: torch.Tensor,
                          return_dict: bool = False) -> torch.Tensor:
        """
        ê°„ë‹¨í•œ ê¶¤ì  ì˜ˆì¸¡ ì¸í„°í˜ì´ìŠ¤
        
        Args:
            bev_features: [B, H*W, C]
            ego_features: [B, C]  
            return_dict: ì „ì²´ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ ì—¬ë¶€
            
        Returns:
            trajectory [B, num_steps, 2] ë˜ëŠ” ì „ì²´ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        with torch.no_grad():
            result = self.forward(bev_features, ego_features)
            
        if return_dict:
            return result
        else:
            return result['trajectory']


def create_mock_trajectory_target(batch_size: int = 1, 
                                num_steps: int = 6,
                                straight_distance: float = 15.0) -> torch.Tensor:
    """
    í…ŒìŠ¤íŠ¸ìš© ì§ì§„ ê¶¤ì  ìƒì„±
    
    Args:
        batch_size: ë°°ì¹˜ í¬ê¸°
        num_steps: ë¯¸ë˜ ìŠ¤í… ìˆ˜
        straight_distance: ì´ ì§ì§„ ê±°ë¦¬
        
    Returns:
        [B, num_steps, 2] í˜•íƒœì˜ ê¶¤ì 
    """
    # ì§ì§„ ê¶¤ì  (y=0, xëŠ” ë“±ê°„ê²© ì¦ê°€)
    x_coords = torch.linspace(0, straight_distance, num_steps)
    y_coords = torch.zeros(num_steps)
    
    trajectory = torch.stack([x_coords, y_coords], dim=-1)  # [num_steps, 2]
    trajectory = trajectory.unsqueeze(0).expand(batch_size, -1, -1)  # [B, num_steps, 2]
    
    return trajectory


if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª SimplifiedBEVPlanner í…ŒìŠ¤íŠ¸...")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    planner = SimplifiedBEVPlanner(
        bev_embed_dim=256,
        ego_embed_dim=256,
        hidden_dim=512,
        num_future_steps=6
    )
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    batch_size = 2
    bev_features = torch.randn(batch_size, 3840, 256)  # 48*80=3840
    ego_features = torch.randn(batch_size, 256)
    
    # ì˜ˆì¸¡ ì‹¤í–‰
    with torch.no_grad():
        result = planner(bev_features, ego_features)
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"   - ê¶¤ì  í¬ê¸°: {result['trajectory'].shape}")
    print(f"   - ì‹ ë¢°ë„ í¬ê¸°: {result['confidence'].shape}")
    print(f"   - ê¶¤ì  ë²”ìœ„: [{result['trajectory'].min():.3f}, {result['trajectory'].max():.3f}]")
    print(f"   - ì‹ ë¢°ë„ ë²”ìœ„: [{result['confidence'].min():.3f}, {result['confidence'].max():.3f}]") 