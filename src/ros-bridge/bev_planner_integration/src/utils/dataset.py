#!/usr/bin/env python3
"""
BEV-Planner í•™ìŠµìš© PyTorch Dataset
"""

import os
import pickle
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import glob
from typing import List, Dict, Tuple
import logging

class BEVPlannerDataset(Dataset):
    """BEV-Planner í•™ìŠµìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, 
                 data_dir: str, 
                 transform=None,
                 max_samples: int = None,
                 val_split: float = 0.2):
        """
        Args:
            data_dir: ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í„°ë¦¬
            transform: ë°ì´í„° ë³€í™˜ í•¨ìˆ˜
            max_samples: ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ (ë©”ëª¨ë¦¬ ì œí•œìš©)
            val_split: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
        """
        self.data_dir = data_dir
        self.transform = transform
        self.max_samples = max_samples
        
        # ë°ì´í„° ë¡œë“œ
        self.samples = self.load_all_data()
        
        if len(self.samples) == 0:
            raise ValueError(f"ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
            
        logging.info(f"ğŸ“Š ì´ {len(self.samples)} ìƒ˜í”Œ ë¡œë“œë¨")
        
        # ë°ì´í„° ê²€ì¦
        self.validate_data()
        
    def load_all_data(self) -> List[Dict]:
        """ëª¨ë“  pickle íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        all_samples = []
        
        # ëª¨ë“  .pkl íŒŒì¼ ì°¾ê¸°
        pkl_files = glob.glob(os.path.join(self.data_dir, "*.pkl"))
        
        if not pkl_files:
            logging.warning(f"âš ï¸  {self.data_dir}ì— .pkl íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return []
            
        for pkl_file in pkl_files:
            try:
                with open(pkl_file, 'rb') as f:
                    data = pickle.load(f)
                    if isinstance(data, list):
                        all_samples.extend(data)
                    else:
                        all_samples.append(data)
                        
                logging.info(f"âœ… {os.path.basename(pkl_file)} ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e:
                logging.error(f"âŒ {pkl_file} ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
                
        # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜ ì œí•œ
        if self.max_samples and len(all_samples) > self.max_samples:
            all_samples = all_samples[:self.max_samples]
            logging.info(f"ğŸ”¢ ìƒ˜í”Œ ìˆ˜ë¥¼ {self.max_samples}ê°œë¡œ ì œí•œ")
            
        return all_samples
        
    def validate_data(self):
        """ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
        required_keys = [
            'det_grid', 'da_grid', 'll_grid',
            'ego_velocity', 'ego_angular_velocity', 
            'expert_trajectory'
        ]
        
        valid_samples = []
        
        for sample in self.samples:
            # í•„ìˆ˜ í‚¤ í™•ì¸
            if not all(key in sample for key in required_keys):
                continue
                
            # ë°ì´í„° íƒ€ì… ë° í¬ê¸° í™•ì¸
            try:
                # BEV grids: (48, 80) í¬ê¸°
                for grid_key in ['det_grid', 'da_grid', 'll_grid']:
                    grid = sample[grid_key]
                    if not isinstance(grid, np.ndarray) or grid.shape != (48, 80):
                        raise ValueError(f"ì˜ëª»ëœ ê·¸ë¦¬ë“œ í¬ê¸°: {grid.shape}")
                        
                # Expert trajectory: (6, 2) í¬ê¸°
                traj = sample['expert_trajectory']
                if not isinstance(traj, np.ndarray) or traj.shape != (6, 2):
                    raise ValueError(f"ì˜ëª»ëœ ê¶¤ì  í¬ê¸°: {traj.shape}")
                    
                # Ego status: ìŠ¤ì¹¼ë¼ ê°’ë“¤
                if not isinstance(sample['ego_velocity'], (int, float)):
                    raise ValueError("ì˜ëª»ëœ ì†ë„ ë°ì´í„°")
                    
                if not isinstance(sample['ego_angular_velocity'], (int, float)):
                    raise ValueError("ì˜ëª»ëœ ê°ì†ë„ ë°ì´í„°")
                    
                valid_samples.append(sample)
                
            except Exception as e:
                logging.warning(f"âš ï¸  ìƒ˜í”Œ ê²€ì¦ ì‹¤íŒ¨: {e}")
                continue
                
        logging.info(f"âœ… {len(valid_samples)}/{len(self.samples)} ìƒ˜í”Œì´ ìœ íš¨í•¨")
        self.samples = valid_samples
        
    def __len__(self) -> int:
        return len(self.samples)
        
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Returns:
            bev_features: (3, 48, 80) - det, da, ll ê·¸ë¦¬ë“œë“¤
            ego_features: (2,) - velocity, angular_velocity
            target_trajectory: (6, 2) - ëª©í‘œ ê¶¤ì 
        """
        sample = self.samples[idx]
        
        # BEV features: (3, 48, 80)
        det_grid = sample['det_grid']
        da_grid = sample['da_grid'] 
        ll_grid = sample['ll_grid']
        
        bev_features = np.stack([det_grid, da_grid, ll_grid], axis=0)
        bev_features = torch.from_numpy(bev_features).float()
        
        # Ego features: (2,)
        ego_features = np.array([
            sample['ego_velocity'],
            sample['ego_angular_velocity']
        ])
        ego_features = torch.from_numpy(ego_features).float()
        
        # Target trajectory: (6, 2)
        target_trajectory = sample['expert_trajectory']
        target_trajectory = torch.from_numpy(target_trajectory).float()
        
        # ë°ì´í„° ë³€í™˜ ì ìš©
        if self.transform:
            bev_features, ego_features, target_trajectory = self.transform(
                bev_features, ego_features, target_trajectory
            )
            
        return bev_features, ego_features, target_trajectory
        
    def get_statistics(self) -> Dict:
        """ë°ì´í„°ì…‹ í†µê³„ ì •ë³´"""
        if not self.samples:
            return {}
            
        velocities = [s['ego_velocity'] for s in self.samples]
        angular_vels = [s['ego_angular_velocity'] for s in self.samples]
        
        trajectories = np.array([s['expert_trajectory'] for s in self.samples])
        
        stats = {
            'num_samples': len(self.samples),
            'velocity_mean': np.mean(velocities),
            'velocity_std': np.std(velocities),
            'angular_velocity_mean': np.mean(angular_vels),
            'angular_velocity_std': np.std(angular_vels),
            'trajectory_mean': np.mean(trajectories, axis=0),
            'trajectory_std': np.std(trajectories, axis=0),
            'trajectory_max_distance': np.max(np.linalg.norm(trajectories, axis=-1))
        }
        
        return stats


def create_dataloaders(data_dir: str, 
                      batch_size: int = 32,
                      val_split: float = 0.2,
                      num_workers: int = 4,
                      max_samples: int = None) -> Tuple[DataLoader, DataLoader]:
    """í•™ìŠµ/ê²€ì¦ DataLoader ìƒì„±"""
    
    # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
    full_dataset = BEVPlannerDataset(
        data_dir=data_dir,
        max_samples=max_samples
    )
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    total_samples = len(full_dataset)
    val_samples = int(total_samples * val_split)
    train_samples = total_samples - val_samples
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_samples, val_samples],
        generator=torch.Generator().manual_seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ë¶„í• 
    )
    
    # DataLoader ìƒì„±
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    logging.info(f"ğŸ“š í•™ìŠµ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ")
    logging.info(f"ğŸ“– ê²€ì¦ ë°ì´í„°: {len(val_dataset)} ìƒ˜í”Œ")
    
    return train_loader, val_loader


# ë°ì´í„° ë³€í™˜ í•¨ìˆ˜ë“¤
class BEVNormalize:
    """BEV ê·¸ë¦¬ë“œ ì •ê·œí™”"""
    def __init__(self, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)):
        self.mean = torch.tensor(mean).view(3, 1, 1)
        self.std = torch.tensor(std).view(3, 1, 1)
        
    def __call__(self, bev_features, ego_features, target_trajectory):
        bev_features = (bev_features - self.mean) / self.std
        return bev_features, ego_features, target_trajectory


class EgoNormalize:
    """Ego ìƒíƒœ ì •ê·œí™”"""
    def __init__(self, velocity_scale=10.0, angular_scale=1.0):
        self.velocity_scale = velocity_scale
        self.angular_scale = angular_scale
        
    def __call__(self, bev_features, ego_features, target_trajectory):
        ego_features[0] = ego_features[0] / self.velocity_scale  # velocity
        ego_features[1] = ego_features[1] / self.angular_scale   # angular velocity
        return bev_features, ego_features, target_trajectory


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import logging
    logging.basicConfig(level=logging.INFO)
    
    data_dir = os.path.expanduser("~/capstone_2025/training_data")
    
    if os.path.exists(data_dir):
        try:
            dataset = BEVPlannerDataset(data_dir)
            print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì„±ê³µ: {len(dataset)} ìƒ˜í”Œ")
            
            # ì²« ë²ˆì§¸ ìƒ˜í”Œ í™•ì¸
            if len(dataset) > 0:
                bev, ego, traj = dataset[0]
                print(f"BEV features shape: {bev.shape}")
                print(f"Ego features shape: {ego.shape}")
                print(f"Target trajectory shape: {traj.shape}")
                
                # í†µê³„ ì¶œë ¥
                stats = dataset.get_statistics()
                for key, value in stats.items():
                    print(f"{key}: {value}")
                    
        except Exception as e:
            print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
    else:
        print(f"âŒ ë°ì´í„° ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        print("ğŸ’¡ ë¨¼ì € data_collector_node.pyë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”!") 