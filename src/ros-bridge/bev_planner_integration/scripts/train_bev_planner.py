#!/usr/bin/env python3
"""
BEV-Planner ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
CARLAì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¡œ BEV-Plannerë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
"""

import os
import sys
import argparse
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from models.simplified_planner import SimplifiedBEVPlanner
from models.safety_checker import PlanningLoss
from utils.dataset import create_dataloaders, BEVNormalize, EgoNormalize
from adapters.yolop_adapter import YOLOPToBEVAdapter

class BEVPlannerTrainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í„°ë¦¬ ì„¤ì •
        self.checkpoint_dir = os.path.join(config['save_dir'], 'checkpoints')
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # TensorBoard ì„¤ì •
        self.writer = SummaryWriter(
            log_dir=os.path.join(config['save_dir'], 'tensorboard')
        )
        
        self.logger.info(f"ğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        self.logger.info(f"ğŸ“ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ: {self.checkpoint_dir}")
        
        # ëª¨ë¸, ë°ì´í„°, ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        self.setup_model()
        self.setup_data()
        self.setup_optimizer()
        self.setup_loss()
        
        # í•™ìŠµ ìƒíƒœ
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.global_step = 0
        
    def setup_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        # YOLOP adapter
        self.adapter = YOLOPToBEVAdapter(
            input_height=48,
            input_width=80,
            embed_dim=self.config['bev_embed_dim']
        ).to(self.device)
        
        # BEV Planner
        self.model = SimplifiedBEVPlanner(
            bev_embed_dim=self.config['bev_embed_dim'],
            ego_embed_dim=self.config['ego_embed_dim'],
            hidden_dim=self.config['hidden_dim'],
            num_future_steps=self.config['num_future_steps'],
            num_transformer_layers=self.config['num_transformer_layers'],
            num_attention_heads=self.config['num_attention_heads'],
            dropout=self.config['dropout']
        ).to(self.device)
        
        self.logger.info(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        self.logger.info(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def setup_data(self):
        """ë°ì´í„° ë¡œë” ì„¤ì •"""
        # ë°ì´í„° ë³€í™˜
        transform = None
        if self.config.get('normalize', True):
            # TODO: ì‹¤ì œ ë°ì´í„° í†µê³„ ê¸°ë°˜ìœ¼ë¡œ ì •ê·œí™” ê°’ ê³„ì‚°
            transform = lambda bev, ego, traj: (
                BEVNormalize()(bev, ego, traj)[0:1] + 
                EgoNormalize()(bev, ego, traj)[1:3]
            )
        
        self.train_loader, self.val_loader = create_dataloaders(
            data_dir=self.config['data_dir'],
            batch_size=self.config['batch_size'],
            val_split=self.config['val_split'],
            num_workers=self.config['num_workers'],
            max_samples=self.config.get('max_samples')
        )
        
        self.logger.info(f"ğŸ“š í•™ìŠµ ë°°ì¹˜ ìˆ˜: {len(self.train_loader)}")
        self.logger.info(f"ğŸ“– ê²€ì¦ ë°°ì¹˜ ìˆ˜: {len(self.val_loader)}")
        
    def setup_optimizer(self):
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        # ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„° í•©ì¹˜ê¸°
        all_parameters = list(self.adapter.parameters()) + list(self.model.parameters())
        
        self.optimizer = optim.AdamW(
            all_parameters,
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=self.config['scheduler_patience'],
            verbose=True
        )
        
        self.logger.info(f"âš™ï¸  ì˜µí‹°ë§ˆì´ì €: AdamW (lr={self.config['learning_rate']})")
        
    def setup_loss(self):
        """ì†ì‹¤ í•¨ìˆ˜ ì„¤ì •"""
        self.planning_loss = PlanningLoss(
            trajectory_weight=self.config['trajectory_weight'],
            collision_weight=self.config['collision_weight'],
            lane_keeping_weight=self.config['lane_keeping_weight'],
            smoothness_weight=self.config['smoothness_weight'],
            confidence_weight=self.config['confidence_weight']
        )
        
        self.mse_loss = nn.MSELoss()
        
        self.logger.info("âœ… ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • ì™„ë£Œ")
        
    def train_epoch(self):
        """í•œ ì—í¬í¬ í•™ìŠµ"""
        self.adapter.train()
        self.model.train()
        
        total_loss = 0.0
        num_batches = len(self.train_loader)
        
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch+1}")
        
        for batch_idx, (bev_features, ego_features, target_trajectory) in enumerate(progress_bar):
            # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
            bev_features = bev_features.to(self.device)  # (B, 3, 48, 80)
            ego_features = ego_features.to(self.device)  # (B, 2)
            target_trajectory = target_trajectory.to(self.device)  # (B, 6, 2)
            
            # Forward pass
            self.optimizer.zero_grad()
            
            # Ego statusë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            ego_status_dict = {
                'velocity': [ego_features[0, 0].item(), 0.0],  # [vx, vy]
                'steering': 0.0,
                'yaw_rate': ego_features[0, 1].item(),  # angular_velocity
                'acceleration': 0.0
            }
            
            # YOLOP featuresë¥¼ BEV featuresë¡œ ë³€í™˜
            adapted_bev_features = self.adapter(
                detection_mask=bev_features[:, 0],  # (B, 48, 80)
                drivable_area_mask=bev_features[:, 1],   # (B, 48, 80)
                lane_line_mask=bev_features[:, 2],   # (B, 48, 80)
                ego_status=ego_status_dict
            )
            
            # BEV-Planner ì¶”ë¡ 
            outputs = self.model(
                bev_features=adapted_bev_features['bev_features'],
                ego_features=ego_features
            )
            
            predicted_trajectory = outputs['trajectory']  # (B, 6, 2)
            predicted_confidence = outputs['confidence']  # (B, 6)
            
            # ì†ì‹¤ ê³„ì‚°
            # 1. MSE Loss (ê¸°ë³¸ ê¶¤ì  ë§¤ì¹­)
            mse_loss = self.mse_loss(predicted_trajectory, target_trajectory)
            
            # 2. Planning Loss (ë¬¼ë¦¬ì  ì œì•½, ì¶©ëŒ ë“±)  
            planning_loss_value = self.planning_loss(
                predicted_trajectory=predicted_trajectory,
                predicted_confidence=predicted_confidence,
                target_trajectory=target_trajectory,
                detection_mask=bev_features[:, 0] > 0.5,  # (B, 48, 80)
                drivable_area_mask=bev_features[:, 1] > 0.5,  # (B, 48, 80)
                lane_line_mask=bev_features[:, 2] > 0.5   # (B, 48, 80)
            )
            
            # ì´ ì†ì‹¤
            total_loss_batch = mse_loss + planning_loss_value['total_loss']
            
            # Backward pass
            total_loss_batch.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
            torch.nn.utils.clip_grad_norm_(
                list(self.adapter.parameters()) + list(self.model.parameters()),
                max_norm=self.config['grad_clip']
            )
            
            self.optimizer.step()
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            total_loss += total_loss_batch.item()
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'Loss': f'{total_loss_batch.item():.4f}',
                'MSE': f'{mse_loss.item():.4f}',
                'Plan': f'{planning_loss_value["total_loss"].item():.4f}',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.6f}'
            })
            
            # TensorBoard ë¡œê¹…
            if self.global_step % self.config['log_interval'] == 0:
                self.writer.add_scalar('Train/Total_Loss', total_loss_batch.item(), self.global_step)
                self.writer.add_scalar('Train/MSE_Loss', mse_loss.item(), self.global_step)
                self.writer.add_scalar('Train/Planning_Loss', planning_loss_value['total_loss'].item(), self.global_step)
                self.writer.add_scalar('Train/Learning_Rate', self.optimizer.param_groups[0]['lr'], self.global_step)
                
                # ì„¸ë¶€ planning lossë“¤ë„ ë¡œê¹…
                for loss_name, loss_value in planning_loss_value.items():
                    if loss_name != 'total_loss' and loss_value is not None:
                        self.writer.add_scalar(f'Train/Planning_{loss_name}', loss_value.item(), self.global_step)
                
            self.global_step += 1
            
        avg_loss = total_loss / num_batches
        self.logger.info(f"ğŸ“Š Epoch {self.current_epoch+1} í•™ìŠµ ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        
        return avg_loss
        
    def validate(self):
        """ê²€ì¦"""
        self.adapter.eval()
        self.model.eval()
        
        total_loss = 0.0
        num_batches = len(self.val_loader)
        
        with torch.no_grad():
            for bev_features, ego_features, target_trajectory in self.val_loader:
                # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
                bev_features = bev_features.to(self.device)
                ego_features = ego_features.to(self.device)
                target_trajectory = target_trajectory.to(self.device)
                
                # Ego statusë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
                ego_status_dict = {
                    'velocity': [ego_features[0, 0].item(), 0.0],  # [vx, vy]
                    'steering': 0.0,
                    'yaw_rate': ego_features[0, 1].item(),  # angular_velocity
                    'acceleration': 0.0
                }
                
                # Forward pass
                adapted_bev_features = self.adapter(
                    detection_mask=bev_features[:, 0],
                    drivable_area_mask=bev_features[:, 1],
                    lane_line_mask=bev_features[:, 2],
                    ego_status=ego_status_dict
                )
                
                outputs = self.model(
                    bev_features=adapted_bev_features['bev_features'],
                    ego_features=ego_features
                )
                
                predicted_trajectory = outputs['trajectory']
                predicted_confidence = outputs['confidence']
                
                # ì†ì‹¤ ê³„ì‚°
                mse_loss = self.mse_loss(predicted_trajectory, target_trajectory)
                
                planning_loss_value = self.planning_loss(
                    predicted_trajectory=predicted_trajectory,
                    predicted_confidence=predicted_confidence,
                    target_trajectory=target_trajectory,
                    detection_mask=bev_features[:, 0] > 0.5,
                    drivable_area_mask=bev_features[:, 1] > 0.5,
                    lane_line_mask=bev_features[:, 2] > 0.5
                )
                
                total_loss_batch = mse_loss + planning_loss_value['total_loss']
                total_loss += total_loss_batch.item()
                
        avg_val_loss = total_loss / num_batches
        self.logger.info(f"ğŸ” ê²€ì¦ ì™„ë£Œ - í‰ê·  ì†ì‹¤: {avg_val_loss:.4f}")
        
        # TensorBoard ë¡œê¹…
        self.writer.add_scalar('Val/Loss', avg_val_loss, self.current_epoch)
        
        return avg_val_loss
        
    def save_checkpoint(self, is_best=False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': self.current_epoch,
            'adapter_state_dict': self.adapter.state_dict(),
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'global_step': self.global_step,
            'config': self.config
        }
        
        # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        checkpoint_path = os.path.join(self.checkpoint_dir, 'latest_checkpoint.pth')
        torch.save(checkpoint, checkpoint_path)
        
        # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if is_best:
            best_path = os.path.join(self.checkpoint_dir, 'best_checkpoint.pth')
            torch.save(checkpoint, best_path)
            self.logger.info(f"ğŸ† ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {best_path}")
            
        # ì—í¬í¬ë³„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì„ íƒì )
        if self.current_epoch % self.config['save_interval'] == 0:
            epoch_path = os.path.join(self.checkpoint_dir, f'checkpoint_epoch_{self.current_epoch}.pth')
            torch.save(checkpoint, epoch_path)
            
    def load_checkpoint(self, checkpoint_path):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
        if not os.path.exists(checkpoint_path):
            self.logger.warning(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
            return False
            
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.adapter.load_state_dict(checkpoint['adapter_state_dict'])
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            self.current_epoch = checkpoint['epoch']
            self.best_val_loss = checkpoint['best_val_loss']
            self.global_step = checkpoint['global_step']
            
            self.logger.info(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ: epoch {self.current_epoch}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
            
    def train(self):
        """ë©”ì¸ í•™ìŠµ ë£¨í”„"""
        self.logger.info("ğŸš€ BEV-Planner í•™ìŠµ ì‹œì‘!")
        
        # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘ (ì˜µì…˜)
        if self.config.get('resume_from_checkpoint'):
            self.load_checkpoint(self.config['resume_from_checkpoint'])
            
        try:
            for epoch in range(self.current_epoch, self.config['num_epochs']):
                self.current_epoch = epoch
                
                # í•™ìŠµ
                train_loss = self.train_epoch()
                
                # ê²€ì¦
                val_loss = self.validate()
                
                # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
                self.scheduler.step(val_loss)
                
                # ìµœê³  ì„±ëŠ¥ í™•ì¸
                is_best = val_loss < self.best_val_loss
                if is_best:
                    self.best_val_loss = val_loss
                    
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                self.save_checkpoint(is_best=is_best)
                
                self.logger.info(f"ğŸ¯ Epoch {epoch+1} ì™„ë£Œ - Train: {train_loss:.4f}, Val: {val_loss:.4f}")
                
        except KeyboardInterrupt:
            self.logger.info("â¹ï¸  í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤")
        except Exception as e:
            import traceback
            self.logger.error(f"âŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.logger.error(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        finally:
            self.writer.close()
            
        self.logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        self.logger.info(f"ğŸ† ìµœê³  ê²€ì¦ ì†ì‹¤: {self.best_val_loss:.4f}")


def main():
    parser = argparse.ArgumentParser(description='BEV-Planner í•™ìŠµ')
    parser.add_argument('--config', type=str, default='config/train_config.json',
                       help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--data_dir', type=str, 
                       default=os.path.expanduser('~/capstone_2025/training_data'),
                       help='í•™ìŠµ ë°ì´í„° ë””ë ‰í„°ë¦¬')
    parser.add_argument('--save_dir', type=str,
                       default=os.path.expanduser('~/capstone_2025/training_results'),
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬')
    
    args = parser.parse_args()
    
    # ê¸°ë³¸ ì„¤ì •
    default_config = {
        'data_dir': args.data_dir,
        'save_dir': args.save_dir,
        'batch_size': 8,   # GPU ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¤„ì„
        'num_epochs': 50,  # ì´ˆê¸° í•™ìŠµì€ 50 ì—í¬í¬ë¡œ ì‹œì‘
        'learning_rate': 5e-5,  # ì¡°ê¸ˆ ë” ë³´ìˆ˜ì ì¸ í•™ìŠµë¥ 
        'weight_decay': 1e-4,   # ì •ê·œí™” ê°•í™”
        'val_split': 0.2,
        'num_workers': 2,  # ì•ˆì •ì„±ì„ ìœ„í•´ ì¤„ì„
        
        # ëª¨ë¸ ì„¤ì •
        'bev_embed_dim': 256,
        'ego_embed_dim': 2,  # ego_featuresëŠ” [velocity, angular_velocity] 2ì°¨ì›
        'hidden_dim': 512,
        'num_future_steps': 6,
        'num_transformer_layers': 4,
        'num_attention_heads': 8,
        'dropout': 0.1,
        
        # ì†ì‹¤ ê°€ì¤‘ì¹˜ (ê°ì†ë„ ê°œì„ ì„ ìœ„í•´ ì¡°ì •)
        'trajectory_weight': 2.0,     # ê¶¤ì  ì •í™•ë„ ê°•í™”
        'collision_weight': 8.0,      # ì•ˆì „ì„± ìœ ì§€í•˜ë˜ ë„ˆë¬´ ê³¼í•˜ì§€ ì•Šê²Œ
        'lane_keeping_weight': 3.0,   # ì°¨ì„  ìœ ì§€ ì ë‹¹íˆ
        'smoothness_weight': 5.0,     # ë¶€ë“œëŸ¬ìš´ ê²½ë¡œ ìƒì„± ê°•í™”
        'confidence_weight': 0.5,     # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì¦ê°€
        
        # ê¸°íƒ€
        'grad_clip': 0.5,       # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ê°•í™”
        'log_interval': 20,     # ë” ìì£¼ ë¡œê¹…
        'save_interval': 5,     # ë” ìì£¼ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        'scheduler_patience': 3, # í•™ìŠµë¥  ê°ì†Œ ì¡°ê±´ ì—„ê²©í™”
        'max_samples': None,
        'normalize': True,
        'early_stopping_patience': 10  # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ì¶”ê°€
    }
    
    # ì„¤ì • íŒŒì¼ ë¡œë“œ (ì¡´ì¬í•  ê²½ìš°)
    if os.path.exists(args.config):
        with open(args.config, 'r') as f:
            file_config = json.load(f)
            default_config.update(file_config)
    
    # ë””ë ‰í„°ë¦¬ ìƒì„±
    os.makedirs(default_config['save_dir'], exist_ok=True)
    
    # ì„¤ì • ì €ì¥
    config_save_path = os.path.join(default_config['save_dir'], 'config.json')
    with open(config_save_path, 'w') as f:
        json.dump(default_config, f, indent=2)
    
    # í•™ìŠµ ì‹œì‘
    trainer = BEVPlannerTrainer(default_config)
    trainer.train()


if __name__ == '__main__':
    main() 